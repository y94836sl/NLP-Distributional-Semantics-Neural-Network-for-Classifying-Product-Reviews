{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rubyli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/rubyli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/rubyli/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# For file reading\n",
    "import os\n",
    "from os import listdir\n",
    "# For pre-processing \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e4e155-2070-4b3e-ac7b-59e70c37ceb5",
   "metadata": {},
   "source": [
    "# Task 1 - Step 1 - Pre-processing words and get top 50 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bad112-f093-4a91-9de6-e67a56cdc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> list:\n",
    "    \"\"\" Read files from a directory and then append the data of each file into a list. \"\"\"\n",
    "    folder = listdir(path)\n",
    "    res = []\n",
    "    for files in folder:\n",
    "        # check if current path is a file\n",
    "        if files != \"README.txt\":\n",
    "            filePath = os.path.join(path, files)\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                res.append(lines)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9c38aa-5974-4136-9dc0-0996730a0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def process_document(document: str) -> list:\n",
    "        \"\"\" pre-process a document and return a list of its terms: str->list\"\"\"\n",
    "        \n",
    "        # Remove number\n",
    "        text_nonum = re.sub(r'\\d+', ' ', document)\n",
    "        \n",
    "        pattern = r'''(?x)        # set flag to allow verbose regexps\n",
    "                    (?:[A-Z]\\.)+     #abbreviations\n",
    "                    |\\[\n",
    "                    |[^\\w\\s]\n",
    "                    |\\#\n",
    "                    |[-.(]+           #double hyphen, ellipsis, open parenthesis\n",
    "                    |\\S\\w*\n",
    "                    |\\$?\\d+(?:\\.\\d+)?%? #currency and percentages\n",
    "        '''\n",
    "        #Tokenization\n",
    "        tokenList = nltk.regexp_tokenize(text_nonum, pattern)\n",
    "        #To lower case\n",
    "        tokenList = [word.lower() for word in tokenList]\n",
    "        #Remove Punctuation\n",
    "        tokenList = list(filter(lambda word: punkt.PunktToken(word).is_non_punct,tokenList))\n",
    "        #Remove stopwords\n",
    "        stopW = stopwords.words(\"english\")\n",
    "        stopW.append(\"u\")\n",
    "        stopW.append(\"p\")\n",
    "        # stopW.append(\"mp\")\n",
    "        tokenList = list(filter(lambda word: word not in stopW,tokenList))\n",
    "        # Lemmatisation \n",
    "        lemma = WordNetLemmatizer()\n",
    "        tokenList = [lemma.lemmatize(word) for word in tokenList]\n",
    "\n",
    "        return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeaebacc-04e8-4736-96ad-2b5a34c19981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top50(res: list) -> list:\n",
    "    doc = \"\"\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            doc += b\n",
    "    # Pre-process documents        \n",
    "    producedDoc = process_document(doc)\n",
    "    # Find the 50 most frequently occurred words\n",
    "    # Get the frequency of each word\n",
    "    word_frequencies = FreqDist(producedDoc)\n",
    "    # Sort the dictionary by frequency\n",
    "    sorted_frequencies = sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Select the top 50 words\n",
    "    target_words = [item[0] for item in sorted_frequencies[:50]]\n",
    "    \n",
    "    return target_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b02ead-22ba-486c-bae0-f2e48d1d4693",
   "metadata": {},
   "source": [
    "# Task 1 - Step 2 - Sample and Pseudowords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc9d86-d798-47f8-b6de-f0cf440a288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pseudowords(target_words: list) -> list:\n",
    "    # Sample half of the target words\n",
    "    sample_size = len(target_words) // 2\n",
    "    sample = random.sample(target_words, sample_size)\n",
    "    # Create pseudowords for the sampled target words\n",
    "    madeups = [word[::-1] for word in sample]\n",
    "    # Replace the sampled occurrences of the target words with their pseudowords\n",
    "    pseudowords = target_words[:]\n",
    "    for i, word in enumerate(target_words):\n",
    "        if word in sample:\n",
    "            # Find the index of the word to be replace\n",
    "            replacement_index = sample.index(word)\n",
    "            # Replace the word with the corresponding word from the replacement list\n",
    "            pseudowords[i] = madeups[replacement_index]\n",
    "    return pseudowords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d54af-2db9-477c-8577-32571bde9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "res = read_data(path)\n",
    "doc = \"\"\n",
    "for a in res:\n",
    "    for b in a:\n",
    "        doc += b\n",
    "# print(res[0][:5])\n",
    "target_words = get_top50(res)\n",
    "print(\"Target Words: \\n\",target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ede56-b821-4fac-90b3-21ff7aba49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_words = pseudowords(target_words)\n",
    "print(\"Pseudo Words: \\n\",pseudo_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14119467-822c-4ef7-ba01-fd2fb28599b8",
   "metadata": {},
   "source": [
    "# Task 1 - Step 3 - Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ed61c-f1a1-42f2-8420-2a0823d89150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus containing the target words and pseudowords\n",
    "corpus = target_words + pseudowords\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a8394-146d-49a2-b63a-feef0e1642fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the sentences in all documents\n",
    "sentences = []\n",
    "for doc in res:\n",
    "    for i, sen in enumerate(doc):\n",
    "        pos = {}\n",
    "        process_sen = process_document(sen)\n",
    "        for j, word in enumerate(process_sen):\n",
    "            key = j\n",
    "            if word == \"\":\n",
    "                pos[key] = \"null\"\n",
    "            else:\n",
    "                pos[key] = word\n",
    "        sentences.append(pos)\n",
    "print(sentences[:10])\n",
    "print(len(sentences))\n",
    "print(len(sentences[1].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30db0d-c73d-494f-84d0-405aa82d042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for va in sentences[1].values():\n",
    "    print(va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1234c5-7bdb-4497-b543-3a0643225f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae0463-b1fd-468c-87cb-064b109eee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each sentence and count the frequency of each word\n",
    "from collections import Counter\n",
    "\n",
    "# window_size = 5\n",
    "# sen_count = []\n",
    "# for tword in corpus:\n",
    "#     count = 0\n",
    "#     for i, dic in enumerate(sentences):\n",
    "#         if dic!={} and tword in dic.values():\n",
    "#             numkeys = len(sentences[1].keys())\n",
    "#             if numkeys >= window_size:\n",
    "#                 pos = dic.get(tword)\n",
    "#                 for j in range(window_size):\n",
    "#                     pos \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341e54d-d36f-4c62-a245-b0c5f9218d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct Nxd array based on the word-sentence frequency\n",
    "pre_M = []\n",
    "for word in corpus:\n",
    "    word_freq = []\n",
    "    for sentence in sen_count:\n",
    "        tempfreq = sentence.get(word)\n",
    "        if tempfreq != None:\n",
    "            word_freq.append(tempfreq)\n",
    "        else:\n",
    "            word_freq.append(0)\n",
    "    pre_M.append(word_freq)\n",
    "M = np.array(pre_M)\n",
    "print(M[0][30:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7c7cc-245f-4f7d-b903-cd30780e2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faccfad-1dac-47e0-83fc-da155792456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f827dd0-89f9-4d3a-8ab8-f84d70b64731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVector(target_words: list, pseudowords: list, res: list) -> np.array:\n",
    "    \n",
    "    # Create a corpus containing the target words and pseudowords\n",
    "    corpus = target_words + pseudowords\n",
    "    \n",
    "    # Get all the sentences in all documents\n",
    "    sentences = []\n",
    "    for doc in res:\n",
    "        for sen in doc:\n",
    "            sentences.append(sen)\n",
    "    \n",
    "    # Loop through each sentence and count the frequency of each word\n",
    "    sen_count = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        counts = dict()\n",
    "        words = process_document(sentence)\n",
    "        for word in words:\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "        sen_count.append(counts)\n",
    "        \n",
    "    # Construct Nxd array based on the word-sentence frequency\n",
    "    pre_M = []\n",
    "    for word in corpus:\n",
    "        word_freq = []\n",
    "        for sentence in sen_count:\n",
    "            tempfreq = sentence.get(word)\n",
    "            if tempfreq != None:\n",
    "                word_freq.append(tempfreq)\n",
    "            else:\n",
    "                word_freq.append(0)\n",
    "        pre_M.append(word_freq)\n",
    "    M = np.array(pre_M)\n",
    "    # svd\n",
    "    u,s,v = np.linalg.svd(M)\n",
    "    M = np.dot(u, np.diag(s))\n",
    "    \n",
    "    # normalization\n",
    "    M = M / np.linalg.norm(M, axis=1)[:, None]\n",
    "    \n",
    "    \n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e30091-3dd8-4d55-bd7a-42b668549f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = featureVector(target_words, pseudowords, res)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f46b9-8301-4bd9-b036-98a5d52c2c29",
   "metadata": {},
   "source": [
    "# Task 1 - Step 4 - input matrix X put them into 50 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89e1dc-c92a-4f65-8f37-5cfb6398fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.cluster import KMeansClusterer\n",
    "\n",
    "# X = M\n",
    "# # Set the number of clusters to 50\n",
    "# num_clusters = 50\n",
    "\n",
    "# # Create a KMeansClusterer instance with the specified number of clusters\n",
    "# clusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, avoid_empty_clusters=True)\n",
    "\n",
    "# # Cluster the words using the feature matrix\n",
    "# clusters = clusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "# # Print the cluster assignments for each word\n",
    "# for i, cluster in enumerate(clusters):\n",
    "#     print(f\"Word {i+1} is in cluster {cluster}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09779a14-e6aa-4fc6-ac4a-095a01cb3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# X = M\n",
    "\n",
    "# # Set the number of clusters to 50\n",
    "# num_clusters = 50\n",
    "\n",
    "# # Create a KMeans instance with the specified number of clusters\n",
    "# kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "\n",
    "# # Cluster the words using the feature matrix\n",
    "# clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# visualClusters = {}\n",
    "# # Print the cluster assignments for each word\n",
    "# for i, cluster in enumerate(clusters):\n",
    "# #    print(f\"Word {i+1} is in cluster {cluster}.\")\n",
    "#     visualClusters[corpus[i]] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93060f-3ee7-4d03-8c28-8ef051ed5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getCluster(X: np.array) -> dict:\n",
    "#     # Set the number of clusters to 50\n",
    "#     num_clusters = 50\n",
    "\n",
    "#     # Create a KMeans instance with the specified number of clusters\n",
    "#     kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "\n",
    "#     # Cluster the words using the feature matrix\n",
    "#     clusters = kmeans.fit_predict(X)\n",
    "\n",
    "#     visualClusters = {}\n",
    "#     # Print the cluster assignments for each word\n",
    "#     for i, cluster in enumerate(clusters):\n",
    "#     #    print(f\"Word {i+1} is in cluster {cluster}.\")\n",
    "#         visualClusters[corpus[i]] = cluster\n",
    "#     return visualClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09403be-61c9-45f9-a0a2-c2fffe06fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCluster(X: np.array) -> np.array:\n",
    "    # Set the number of clusters to 50\n",
    "    num_clusters = 50\n",
    "    \n",
    "    # Create a KMeans instance with the specified number of clusters\n",
    "    # kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "    km = KMeans(n_clusters=50).fit(X)\n",
    "    \n",
    "    # Cluster the words using the feature matrix\n",
    "    # clusters = km.fit_predict(X)\n",
    "    performance = []\n",
    "    \n",
    "    labels = km.labels_\n",
    "    labels.tolist()\n",
    "    performance.append((np.sum(labels[0:50] == labels[50:100])) / len(labels[0:50]))\n",
    "    print(\"target words: \" + str(labels[50:100]))\n",
    "    print(\"pesudo words: \" + str(labels[0:50]))\n",
    "    print(\"Performance: \" + str(performance[0]))\n",
    "        \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c545fe-db42-45dc-aa54-4ab77295f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "getCluster(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4434a48-8569-40b6-93bf-0cc88004d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c24c59-9e22-4915-8797-1192a1240879",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = getCluster(M)\n",
    "print(clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2531614-b026-46e5-b3eb-9d1392bd0c7b",
   "metadata": {},
   "source": [
    "# Task 1 - Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8e302-2700-4375-9dd2-410bae6d0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_cluster = clust[:50]\n",
    "# pseudo_cluster = clust[50:]\n",
    "# print(target_cluster,len(target_cluster),\"\\n\",pseudo_cluster, len(pseudo_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab61d18-3e35-4c02-ba8d-c38ddfad6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pseudowords, len(pseudowords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ceb0c-ca3f-4726-bc53-aac03726da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_words, len(target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf29146-1dbd-4ef1-90b2-0f373c37cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_size = 50\n",
    "# temp_count = 0\n",
    "# for i in range(cluster_size):\n",
    "#     if(target_cluster[i] == pseudo_cluster[i]):\n",
    "#         temp_count += 1\n",
    "# p = temp_count/cluster_size\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75191e-fe7b-4209-9990-754b10976cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(clust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab31dc8-ec8f-4d60-84b4-89f7af9c7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getProbability(cluster: np.array) -> int:\n",
    "#     cluster_size = len(cluster) // 2 \n",
    "#     temp_count = 0\n",
    "#     for i in range(cluster_size):\n",
    "#         if(target_cluster[i] == pseudo_cluster[i]):\n",
    "#             temp_count += 1\n",
    "#     p = temp_count/cluster_size\n",
    "#     return p\n",
    "# getProbability(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d48a7c-292b-403f-8fb2-243e7275522f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
