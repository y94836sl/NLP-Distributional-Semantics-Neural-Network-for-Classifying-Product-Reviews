{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rubyli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# For file reading\n",
    "import os\n",
    "from os import listdir\n",
    "# For pre-processing \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "# For bag of word model\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# For Cluster\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bad112-f093-4a91-9de6-e67a56cdc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> list:\n",
    "    \"\"\" Read files from a directory and then append the data of each file into a list. \"\"\"\n",
    "    folder = listdir(path)\n",
    "    res = []\n",
    "    for files in folder:\n",
    "        # check if current path is a file\n",
    "        if files != \"README.txt\":\n",
    "            filePath = os.path.join(path, files)\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                res.append(lines)\n",
    "    return res\n",
    "\n",
    "def process_document(document: str) -> list:\n",
    "        \"\"\" pre-process a document and return a list of its terms: str->list\"\"\"\n",
    "        \n",
    "        # Remove number\n",
    "        text_nonum = re.sub(r'\\d+', ' ', document)\n",
    "        \n",
    "        pattern = r'''(?x)        # set flag to allow verbose regexps\n",
    "                    (?:[A-Z]\\.)+     #abbreviations\n",
    "                    |\\[\n",
    "                    |[^\\w\\s]\n",
    "                    |\\#\n",
    "                    |[-.(]+           #double hyphen, ellipsis, open parenthesis\n",
    "                    |\\S\\w*\n",
    "                    |\\$?\\d+(?:\\.\\d+)?%? #currency and percentages\n",
    "        '''\n",
    "        #Tokenization\n",
    "        tokenList = nltk.regexp_tokenize(text_nonum, pattern)\n",
    "        #To lower case\n",
    "        tokenList = [word.lower() for word in tokenList]\n",
    "        #Remove Punctuation\n",
    "        tokenList = list(filter(lambda word: punkt.PunktToken(word).is_non_punct,tokenList))\n",
    "        #Remove stopwords\n",
    "        stopW = stopwords.words(\"english\")\n",
    "        stopW.append(\"u\")\n",
    "        stopW.append(\"p\")\n",
    "        # stopW.append(\"mp\")\n",
    "        tokenList = list(filter(lambda word: word not in stopW,tokenList))\n",
    "        # Lemmatisation \n",
    "        lemma = WordNetLemmatizer()\n",
    "        tokenList = [lemma.lemmatize(word) for word in tokenList]\n",
    "\n",
    "        return tokenList \n",
    "\n",
    "\n",
    "def process_reviews_str(res: list) -> list:\n",
    "   # merge all reviews\n",
    "    doc = \"\"\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            doc += b\n",
    "    # Pre-process documents        \n",
    "    producedDoc = process_document(doc)\n",
    "    return producedDoc\n",
    "\n",
    "def process_reviews_list(res: list) -> list:\n",
    "    # store the processed doc in list of sentences \n",
    "    producedDoc = []\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            # Pre-process documents        \n",
    "            producedDoc.append(process_document(b))\n",
    "    return producedDoc\n",
    "\n",
    "\n",
    "def get_top50(producedDoc: list) -> list:\n",
    "    '''Find the 50 most frequently occurred words'''\n",
    "    # Get the frequency of each word\n",
    "    word_frequencies = FreqDist(producedDoc)\n",
    "    # Sort the dictionary by frequency\n",
    "    sorted_frequencies = sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Select the top 50 words\n",
    "    target_words = [item[0] for item in sorted_frequencies[:50]]\n",
    "    \n",
    "    return target_words\n",
    "\n",
    "def pseudowords(target_words: list, processed_corpus: list) -> list:\n",
    "    # Construct a dic to store all the words' position\n",
    "    position_list = []\n",
    "    for sen in processed_corpus:\n",
    "        pos_dic = {}\n",
    "        for i, word in enumerate(sen):\n",
    "            pos_dic[i] = word\n",
    "        position_list.append(pos_dic)\n",
    "\n",
    "    # Sample half of the target words\n",
    "    sample_size = len(target_words) // 2\n",
    "    sample = random.sample(target_words, sample_size)\n",
    "    \n",
    "    # Create reversed words for the sampled target words\n",
    "    madeups = [word[::-1] for word in sample]\n",
    "    \n",
    "    # Replace the sampled reversed words in the target words\n",
    "    pseudowords = target_words[:]\n",
    "    for i, word in enumerate(target_words):\n",
    "        if word in sample:\n",
    "            # Find the index of the word to be replace\n",
    "            replacement_index = sample.index(word)\n",
    "            # Replace the word with the corresponding word from the replacement list\n",
    "            pseudowords[i] = madeups[replacement_index]\n",
    "            \n",
    "    # Get the position of all the sample words\n",
    "    # sample position\n",
    "    sPos = []\n",
    "    for s in sample:\n",
    "        # sentence position\n",
    "        re_pos = []\n",
    "        for i, sentence in enumerate(processed_corpus):\n",
    "            # word position\n",
    "            indices = [j for j, word in enumerate(sentence) if word == s]\n",
    "            re_pos.append(indices)\n",
    "        sPos.append(re_pos)\n",
    "    \n",
    "    # Randomly generate the index of the word to be replace\n",
    "    re_corpus = processed_corpus[:]\n",
    "    for s, sam in enumerate(sPos):\n",
    "        for i, sent in enumerate(sam):\n",
    "            if sent != []:\n",
    "                # replace half of its occurence in corpus with their reversed words\n",
    "                sSize = len(sent) // 2\n",
    "                sIndex = random.sample(sent, sSize)\n",
    "                for ind in sIndex:\n",
    "                    re_corpus[i][ind] = madeups[s]\n",
    "    \n",
    "    return pseudowords, re_corpus\n",
    "\n",
    "def featureVector(corpus_pos: list, sample: str) -> np.array:\n",
    "    # Vectorize sentences\n",
    "    example_sequence = []\n",
    "  \n",
    "    for word in sample:\n",
    "        sequence = []\n",
    "        for sentence in corpus_pos:\n",
    "            word_counts = Counter(sentence)\n",
    "            if word in sentence:\n",
    "                count = word_counts[word]\n",
    "            else:\n",
    "                count = 0\n",
    "            sequence.append(count)\n",
    "        example_sequence.append(sequence)\n",
    "\n",
    "    M = np.array(example_sequence)\n",
    "    \n",
    "    # svd\n",
    "    u, s, v = np.linalg.svd(M)\n",
    "    M = np.dot(u, np.diag(s))\n",
    "    \n",
    "    # normalisation\n",
    "    M = M / np.linalg.norm(M, axis=1)[:, None]\n",
    "    return M\n",
    "\n",
    "def getClusterProb(X: np.array) -> int:\n",
    "    # Set the number of clusters to 50\n",
    "    num_clusters = 50\n",
    "    \n",
    "    # Create a KMeans instance with the specified number of clusters\n",
    "    km = KMeans(n_clusters=50).fit(X)\n",
    "    \n",
    "    labels = km.labels_\n",
    "    labels.tolist()\n",
    "    \n",
    "    accuracy = (np.sum(labels[0:50] == labels[50:100])) / len(labels[0:50])\n",
    "    # print(\"target words label: \" + str(labels[50:100]))\n",
    "    # print(\"pesudo words label: \" + str(labels[0:50]))\n",
    "    # print(\"accuracy: \" + str(accuracy))\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6a3df5-3bc8-4faf-ac52-c33dd96aeaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  [0.92, 0.92, 0.94, 0.9, 0.92, 0.94, 0.88, 0.92, 0.92, 0.86]\n",
      "Mean:  0.9119999999999999\n",
      "Standard Deviation  0.023999999999999997\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "\n",
    "# Reading documents\n",
    "res = read_data(path)\n",
    "processed_str = process_reviews_str(res)\n",
    "processed_list = process_reviews_list(res)\n",
    "\n",
    "target_words = get_top50(processed_str)\n",
    "accuracy = []\n",
    "for i in range(10):\n",
    "    pseudo_words, new_corpus = pseudowords(target_words, processed_list)\n",
    "    sample = target_words + pseudo_words\n",
    "    M = featureVector(new_corpus, sample)\n",
    "    prob = getClusterProb(M)\n",
    "    accuracy.append(prob)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Mean: \", np.mean(accuracy))\n",
    "print(\"Standard Deviation \", np.std(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
