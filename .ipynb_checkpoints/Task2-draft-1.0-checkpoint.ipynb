{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b18efd-fc1f-457f-ae73-34397493211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "import nltk\n",
    "\n",
    "# For file reading\n",
    "import os\n",
    "from os import listdir\n",
    "# For pre-processing \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "# For bag of word model\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# For Cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7e5886-c4ee-44de-89e2-09f3baef009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> list:\n",
    "    \"\"\" Read files from a directory and then append the data of each file into a list. \"\"\"\n",
    "    folder = listdir(path)\n",
    "    res = []\n",
    "    for files in folder:\n",
    "        # check if current path is a file\n",
    "        if files != \"README.txt\":\n",
    "            filePath = os.path.join(path, files)\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                res.append(lines)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8647ef7d-7cf9-42dd-9a95-34152c93839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "\n",
    "# Reading documents\n",
    "res = read_data(path)\n",
    "# print(res[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b27cd6f-1d29-4e19-8994-2025bf3351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting avaliable data for LSTM\n",
    "avaliable_data = []\n",
    "for document in res:\n",
    "    # Remove \\n from the string list\n",
    "    processedList = [s.strip() for s in document]\n",
    "    # Remove [t] tag\n",
    "    processedList = [re.sub('\\[t\\]', '', word) for word in processedList]\n",
    "    # Store values in list\n",
    "    for sentence in processedList:\n",
    "        if not sentence.startswith(\"##\"):\n",
    "            avaliable_data.append(sentence)\n",
    "    \n",
    "for sen in avaliable_data:\n",
    "    if sen == '':\n",
    "        avaliable_data.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80ee4452-e2e3-4e2d-b446-8dd6485b9c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works[+3]##Works great, no odor, and uses regular bags. \n",
      "\n",
      "Diaper Champ[+2]##So far (3 weeks), we've had no problems with the Diaper Champ at all. \n",
      "\n",
      "diapers[+2]##It contains the smell from our baby's diapers and we can use just about any kind of bag inside. \n",
      "\n",
      "odors[+2],refills[+2]##I don't know if we'll run into trouble down the road with odors, but so far I have no complaints and I'm very happy not to have to buy those refills. \n",
      "\n",
      "Diaper Champ[+3]##The Diaper Champ is the best we found! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sen in avaliable_data[:5]:\n",
    "    print(sen, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "480940c8-78d6-4b67-a59c-68824307ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148\n"
     ]
    }
   ],
   "source": [
    "print(len(avaliable_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df7d95f3-6c00-42f4-b9c9-fceb4474a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Works[+3]', 'Works great, no odor, and uses regular bags.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Store review and sentiment seperatly in 2 lists\n",
    "review = []\n",
    "sentiment = []\n",
    "\n",
    "for data in avaliable_data[:1]:\n",
    "    split_data = data.split(\"##\")\n",
    "     \n",
    "    # '''-----------TEST-----------'''\n",
    "    print(split_data)\n",
    "    \n",
    "    left_tag = split_data[0]\n",
    "    right_text = split_data[1]\n",
    "    \n",
    "    \n",
    "    # print(right)\n",
    "    # Get pre-defined sentiment tags\n",
    "    num_list = re.findall(r'[+|-]\\d+', left_tag)\n",
    "    num_list = [num.replace('+', '') for num in num_list]\n",
    "    \n",
    "    for num in num_list:\n",
    "        count = 0\n",
    "        count += int(num)\n",
    "    print(count)\n",
    "    if count >= 0:\n",
    "        sentiment.append(\"positive\")\n",
    "    \n",
    "# print(num_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b939c90-16a9-4645-92bb-5d2fbb31d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f462d6-ff8f-4c7a-ace8-5a306f9e696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# str_list =[]\n",
    "# for data in training_data[:10]:\n",
    "#     if data != '':\n",
    "#         left =data.split(\"##\")\n",
    "#         num = left[0].replace('[', ' ')\n",
    "#         #print(num)\n",
    "#         num = num.replace(']',' ')\n",
    "#         #print(num)\n",
    "#         num = num.split(' ')\n",
    "#         print(num)\n",
    "#         str_list.append(str(num[1])+\" \"+left[1])\n",
    "# print(str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74154607-467b-40bb-9dbd-9610abc5c1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
