{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rubyli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# For file reading\n",
    "import os\n",
    "from os import listdir\n",
    "# For pre-processing \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "# from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bad112-f093-4a91-9de6-e67a56cdc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> list:\n",
    "    \"\"\" Read files from a directory and then append the data of each file into a list. \"\"\"\n",
    "    folder = listdir(path)\n",
    "    res = []\n",
    "    for files in folder:\n",
    "        # check if current path is a file\n",
    "        if files != \"README.txt\":\n",
    "            filePath = os.path.join(path, files)\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                res.append(lines)\n",
    "    return res\n",
    "\n",
    "def process_document(document: str) -> list:\n",
    "    \"\"\" pre-process a document and return a list of its terms: str->list\"\"\"\n",
    "    text = document.lower()\n",
    "    # remove numbers\n",
    "    text_nonum = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    text_p = \"\".join([char for char in text_nonum if char not in string.punctuation])\n",
    "    \n",
    "    words = word_tokenize(text_p)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.append(\"im\")\n",
    "    stop_words.append(\"ive\")\n",
    "    \n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokenList = [lemma.lemmatize(word) for word in filtered_words]\n",
    "    \n",
    "    return tokenList\n",
    "    \n",
    "\n",
    "def get_top50(res: list) -> list:\n",
    "    doc = \"\"\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            doc += b\n",
    "    # Pre-process documents        \n",
    "    producedDoc = process_document(doc)\n",
    "    # Find the 50 most frequently occurred words\n",
    "    # Get the frequency of each word\n",
    "    word_frequencies = FreqDist(producedDoc)\n",
    "    # Sort the dictionary by frequency\n",
    "    sorted_frequencies = sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Select the top 50 words\n",
    "    target_words = [item[0] for item in sorted_frequencies[:50]]\n",
    "    \n",
    "    return target_words\n",
    "\n",
    "def pseudowords(target_words: list) -> list:\n",
    "    # Sample half of the target words\n",
    "    sample_size = len(target_words) // 2\n",
    "    sample = random.sample(target_words, sample_size)\n",
    "    # Create pseudowords for the sampled target words\n",
    "    madeups = [word[::-1] for word in sample]\n",
    "    # Replace the sampled occurrences of the target words with their pseudowords\n",
    "    pseudowords = target_words[:]\n",
    "    for i, word in enumerate(target_words):\n",
    "        if word in sample:\n",
    "            # Find the index of the word to be replace\n",
    "            replacement_index = sample.index(word)\n",
    "            # Replace the word with the corresponding word from the replacement list\n",
    "            pseudowords[i] = madeups[replacement_index]\n",
    "    return pseudowords\n",
    "\n",
    "def featureVector(target_words: list, pseudowords: list, res: list) -> np.array:\n",
    "    \n",
    "    # Create a corpus containing the target words and pseudowords\n",
    "    corpus = target_words + pseudowords\n",
    "    \n",
    "    # Get all the sentences in all documents\n",
    "    sentences = []\n",
    "    for doc in res:\n",
    "        for sen in doc:\n",
    "            sentences.append(sen)\n",
    "    \n",
    "    # Loop through each sentence and count the frequency of each word\n",
    "    sen_count = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        counts = dict()\n",
    "        words = process_document(sentence)\n",
    "        for word in words:\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "        sen_count.append(counts)\n",
    "        \n",
    "    # Construct Nxd array based on the word-sentence frequency\n",
    "    pre_M = []\n",
    "    for word in corpus:\n",
    "        word_freq = []\n",
    "        for sentence in sen_count:\n",
    "            tempfreq = sentence.get(word)\n",
    "            if tempfreq != None:\n",
    "                word_freq.append(tempfreq)\n",
    "            else:\n",
    "                word_freq.append(0)\n",
    "        pre_M.append(word_freq)\n",
    "    M = np.array(pre_M)\n",
    "    \n",
    "    # svd\n",
    "    U, s, V = np.linalg.svd(M)\n",
    "    M = np.dot(U, np.diag(s))\n",
    "    \n",
    "    # norm\n",
    "    M = M / np.linalg.norm(M, axis=1)[:, None]\n",
    "    return M\n",
    "\n",
    "def getCluster(X: np.array) -> np.array:\n",
    "    # Set the number of clusters to 50\n",
    "    num_clusters = 50\n",
    "    \n",
    "    # Create a KMeans instance with the specified number of clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "    \n",
    "    # Cluster the words using the feature matrix\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def getProbability(cluster: np.array) -> int:\n",
    "    cluster_size = len(cluster) // 2 \n",
    "    temp_count = 0\n",
    "    for i in range(cluster_size):\n",
    "        if(target_cluster[i] == pseudo_cluster[i]):\n",
    "            temp_count += 1\n",
    "    p = temp_count/cluster_size\n",
    "    return p\n",
    "\n",
    "# def main():\n",
    "#     path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "#     # Reading documents\n",
    "#     res = read_data(path)\n",
    "#     target_words = get_top50(res)\n",
    "#     pseudo_words = pseudowords(target_words)\n",
    "    \n",
    "#     M = featureVector(target_words, pseudo_words, res)\n",
    "#     clust = getCluster(M)\n",
    "#     print(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ebc9d86-d798-47f8-b6de-f0cf440a288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6a3df5-3bc8-4faf-ac52-c33dd96aeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "# Reading documents\n",
    "res = read_data(path)\n",
    "target_words = get_top50(res)\n",
    "pseudo_words = pseudowords(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e28b58-026b-4bf1-acfb-9a109ea5f7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'phone', 'ipod', 'router', 'one', 'camera', 'player', 'get', 'battery', 'diaper', 'product', 'work', 'like', 'great', 'time', 'problem', 'feature', 'good', 'zen', 'quality', 'dont', 'u', 'would', 'also', 'sound', 'software', 'computer', 'picture', 'really', 'micro', 'well', 'take', 'easy', 'thing', 'used', 'even', 'need', 'first', 'much', 'want', 'bag', 'champ', 'better', 'mp', 'look', 'creative', 'size', 'go', 'music', 'little']\n"
     ]
    }
   ],
   "source": [
    "print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bd72d5f-2cd6-41aa-baf2-19df30f06e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['esu', 'enohp', 'ipod', 'retuor', 'one', 'camera', 'player', 'teg', 'yrettab', 'diaper', 'product', 'work', 'ekil', 'taerg', 'emit', 'problem', 'erutaef', 'good', 'nez', 'ytilauq', 'dont', 'u', 'dluow', 'also', 'dnuos', 'software', 'computer', 'picture', 'yllaer', 'orcim', 'well', 'take', 'ysae', 'thing', 'desu', 'even', 'deen', 'tsrif', 'much', 'tnaw', 'bag', 'pmahc', 'retteb', 'pm', 'look', 'evitaerc', 'size', 'go', 'cisum', 'little']\n"
     ]
    }
   ],
   "source": [
    "print(pseudo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1331a6d-0018-4be6-878b-cdbc588d3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = featureVector(target_words, pseudo_words, res)\n",
    "clust = getCluster(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5d27c-e8ea-493a-a1b4-1a192c2d7b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e1af4a5-9ff3-4d1f-8ab8-09c011f99ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 46  2 16 16  5 35 49 46  8 28  9 21 20  9  6 46  4 29 20 21  1 49 12\n",
      " 20 27 10 18 19 29 38 39  7 23 39 14 19  0 36 49  3  8 13 15 33 26 34 22\n",
      "  2 11 24 44  2 43 16  5 35 42  3  8 28  9 17  0  3  6  3  4 30 31 21  1\n",
      " 40 12 25 27 10 18 37  0 38 39  0 23  0 14 47 42 36  3  3 45 48 32 33 41\n",
      " 34 22  3 11]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(clust)\n",
    "print(len(clust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ddcf02-8c0e-42da-ab7d-6c81b78ddb8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_90709/3850781566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetProbability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_90709/1274538977.py\u001b[0m in \u001b[0;36mgetProbability\u001b[0;34m(cluster)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtemp_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_cluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpseudo_cluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mtemp_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcluster_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_cluster' is not defined"
     ]
    }
   ],
   "source": [
    "p = getProbability(clust)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298b785-4f53-4363-847b-42c647c88c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
