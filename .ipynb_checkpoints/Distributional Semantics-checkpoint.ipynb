{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rubyli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# For file reading\n",
    "import os\n",
    "from os import listdir\n",
    "# For pre-processing \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "# For skip-gram model\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "# import tensorflow as tf\n",
    "\n",
    "# For Cluster\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90bad112-f093-4a91-9de6-e67a56cdc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> list:\n",
    "    \"\"\" Read files from a directory and then append the data of each file into a list. \"\"\"\n",
    "    folder = listdir(path)\n",
    "    res = []\n",
    "    for files in folder:\n",
    "        # check if current path is a file\n",
    "        if files != \"README.txt\":\n",
    "            filePath = os.path.join(path, files)\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                res.append(lines)\n",
    "    return res\n",
    "\n",
    "def process_document(document: str) -> list:\n",
    "        \"\"\" pre-process a document and return a list of its terms: str->list\"\"\"\n",
    "        \n",
    "        # Remove number\n",
    "        text_nonum = re.sub(r'\\d+', ' ', document)\n",
    "        \n",
    "        pattern = r'''(?x)        # set flag to allow verbose regexps\n",
    "                    (?:[A-Z]\\.)+     #abbreviations\n",
    "                    |\\[\n",
    "                    |[^\\w\\s]\n",
    "                    |\\#\n",
    "                    |[-.(]+           #double hyphen, ellipsis, open parenthesis\n",
    "                    |\\S\\w*\n",
    "                    |\\$?\\d+(?:\\.\\d+)?%? #currency and percentages\n",
    "        '''\n",
    "        #Tokenization\n",
    "        tokenList = nltk.regexp_tokenize(text_nonum, pattern)\n",
    "        #To lower case\n",
    "        tokenList = [word.lower() for word in tokenList]\n",
    "        #Remove Punctuation\n",
    "        tokenList = list(filter(lambda word: punkt.PunktToken(word).is_non_punct,tokenList))\n",
    "        #Remove stopwords\n",
    "        stopW = stopwords.words(\"english\")\n",
    "        stopW.append(\"u\")\n",
    "        stopW.append(\"p\")\n",
    "        # stopW.append(\"mp\")\n",
    "        tokenList = list(filter(lambda word: word not in stopW,tokenList))\n",
    "        # Lemmatisation \n",
    "        lemma = WordNetLemmatizer()\n",
    "        tokenList = [lemma.lemmatize(word) for word in tokenList]\n",
    "\n",
    "        return tokenList \n",
    "\n",
    "def process_reviews_str(res: list) -> str:\n",
    "    # merge all reviews\n",
    "    doc = \"\"\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            doc += b\n",
    "    # Pre-process documents        \n",
    "    producedDoc = process_document(doc)\n",
    "    return producedDoc\n",
    "\n",
    "def process_reviews_list(res: list) -> list:\n",
    "    # merge all reviews\n",
    "    producedDoc = []\n",
    "    doc = \"\"\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            doc += b\n",
    "        producedDoc.append(process_document(doc))\n",
    "    return producedDoc\n",
    "\n",
    "def get_top50(producedDoc: str) -> list:\n",
    "    # Find the 50 most frequently occurred words\n",
    "    # Get the frequency of each word\n",
    "    word_frequencies = FreqDist(producedDoc)\n",
    "    # Sort the dictionary by frequency\n",
    "    sorted_frequencies = sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Select the top 50 words\n",
    "    target_words = [item[0] for item in sorted_frequencies[:50]]\n",
    "    \n",
    "    return target_words\n",
    "\n",
    "def pseudowords(target_words: list) -> list:\n",
    "    # Sample half of the target words\n",
    "    sample_size = len(target_words) // 2\n",
    "    sample = random.sample(target_words, sample_size)\n",
    "    # Create pseudowords for the sampled target words\n",
    "    madeups = [word[::-1] for word in sample]\n",
    "    # Replace the sampled occurrences of the target words with their pseudowords\n",
    "    pseudowords = target_words[:]\n",
    "    for i, word in enumerate(target_words):\n",
    "        if word in sample:\n",
    "            # Find the index of the word to be replace\n",
    "            replacement_index = sample.index(word)\n",
    "            # Replace the word with the corresponding word from the replacement list\n",
    "            pseudowords[i] = madeups[replacement_index]\n",
    "    return pseudowords\n",
    "\n",
    "def featureVector(processed_list: list, sample: str) -> np.array:\n",
    "    \n",
    "    # result = process_reviews_list(res)\n",
    "    # # Create a corpus containing the target words and pseudowords\n",
    "    # sample = target_words + pseudowords\n",
    "    \n",
    "    # # Skip-Gram Model\n",
    "    # sg_model = Word2Vec(processed_list, min_count = 1, vector_size = 100, window = 5, sg = 1) \n",
    "    # word_vectors = sg_model.wv\n",
    "    # print(word_vectors.most_similar('use'))\n",
    "    \n",
    "    cbow_model = Word2Vec(processed_list, min_count = 1, vector_size = 100, window = 5)\n",
    "    word_vectors = cbow_model.wv\n",
    "    # print(cbow_model.wv.most_similar('use'))\n",
    "    \n",
    "    pre_M = []\n",
    "    for s in sample:\n",
    "        \n",
    "\n",
    "#     # svd\n",
    "#     # u, s, v = np.linalg.svd(positive_skip_grams)\n",
    "#     u, s, v = np.linalg.svd(word_vectors)\n",
    "#     # u, s, v = np.linalg.svd(M)\n",
    "#     M = np.dot(u, np.diag(s))\n",
    "    \n",
    "#     # norm\n",
    "#     M = M / np.linalg.norm(M, axis=1)[:, None]\n",
    "#     return M\n",
    "\n",
    "def getCluster(X: np.array) -> np.array:\n",
    "    # Set the number of clusters to 50\n",
    "    num_clusters = 50\n",
    "    \n",
    "    # Create a KMeans instance with the specified number of clusters\n",
    "    km = KMeans(n_clusters=50).fit(X)\n",
    "    \n",
    "    performance = []\n",
    "    \n",
    "    labels = km.labels_\n",
    "    labels.tolist()\n",
    "    performance.append((np.sum(labels[0:50] == labels[50:100])) / len(labels[0:50]))\n",
    "    print(\"target words: \" + str(labels[50:100]))\n",
    "    print(\"pesudo words: \" + str(labels[0:50]))\n",
    "    print(\"Performance: \" + str(performance[0]))\n",
    "        \n",
    "    return performance\n",
    "\n",
    "# def getProbability(cluster: np.array) -> int:\n",
    "#     cluster_size = len(cluster) // 2 \n",
    "#     temp_count = 0\n",
    "#     for i in range(cluster_size):\n",
    "#         if(target_cluster[i] == pseudo_cluster[i]):\n",
    "#             temp_count += 1\n",
    "#     p = temp_count/cluster_size\n",
    "#     return p\n",
    "\n",
    "# def main():\n",
    "#     path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "#     # Reading documents\n",
    "#     res = read_data(path)\n",
    "#     target_words = get_top50(res)\n",
    "#     pseudo_words = pseudowords(target_words)\n",
    "    \n",
    "#     M = featureVector(target_words, pseudo_words, res)\n",
    "#     clust = getCluster(M)\n",
    "#     print(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ebc9d86-d798-47f8-b6de-f0cf440a288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb6a3df5-3bc8-4faf-ac52-c33dd96aeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "# Reading documents\n",
    "res = read_data(path)\n",
    "processed_str = process_reviews_str(res)\n",
    "processed_list = process_reviews_list(res)\n",
    "target_words = get_top50(processed_str)\n",
    "pseudo_words = pseudowords(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59e28b58-026b-4bf1-acfb-9a109ea5f7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'phone', 'one', 'router', 'ipod', 'camera', 'player', 'get', 'battery', 'diaper', 'product', 'work', 'like', 'great', 'time', 'feature', 'problem', 'good', 'quality', 'zen', 'would', 'also', 'sound', 'computer', 'software', 'picture', 'well', 'really', 'micro', 'take', 'easy', 'thing', 'even', 'first', 'used', 'need', 'creative', 'bag', 'much', 'want', 'better', 'champ', 'mp', 'look', 'go', 'size', 'music', 'norton', 'little', 'price']\n"
     ]
    }
   ],
   "source": [
    "print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bd72d5f-2cd6-41aa-baf2-19df30f06e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['esu', 'enohp', 'eno', 'router', 'ipod', 'camera', 'reyalp', 'teg', 'yrettab', 'repaid', 'tcudorp', 'krow', 'ekil', 'great', 'time', 'erutaef', 'melborp', 'good', 'quality', 'zen', 'dluow', 'also', 'dnuos', 'computer', 'erawtfos', 'picture', 'llew', 'yllaer', 'orcim', 'ekat', 'easy', 'thing', 'neve', 'first', 'used', 'need', 'evitaerc', 'bag', 'much', 'want', 'better', 'champ', 'mp', 'look', 'go', 'ezis', 'cisum', 'norton', 'elttil', 'ecirp']\n"
     ]
    }
   ],
   "source": [
    "print(pseudo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1331a6d-0018-4be6-878b-cdbc588d3535",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_7259/2998241693.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# M = featureVector(target_words, pseudo_words, res)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# clust = getCluster(M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatureVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_7259/3250233053.py\u001b[0m in \u001b[0;36mfeatureVector\u001b[0;34m(processed_list)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# print(cbow_model.wv.most_similar('use'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m#     # svd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# M = featureVector(target_words, pseudo_words, res)\n",
    "# clust = getCluster(M)\n",
    "featureVector(processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5d27c-e8ea-493a-a1b4-1a192c2d7b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_7259/1779905458.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1af4a5-9ff3-4d1f-8ab8-09c011f99ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ddcf02-8c0e-42da-ab7d-6c81b78ddb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = getProbability(clust)\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298b785-4f53-4363-847b-42c647c88c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
