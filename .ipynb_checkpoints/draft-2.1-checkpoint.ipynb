{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rubyli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# For file reading\n",
    "import os\n",
    "from os import listdir\n",
    "# For pre-processing \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "# For bag of word model\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# For Cluster\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e4e155-2070-4b3e-ac7b-59e70c37ceb5",
   "metadata": {},
   "source": [
    "# Task 1 - Step 1 - Pre-processing words and get top 50 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bad112-f093-4a91-9de6-e67a56cdc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> list:\n",
    "    \"\"\" Read files from a directory and then append the data of each file into a list. \"\"\"\n",
    "    folder = listdir(path)\n",
    "    res = []\n",
    "    for files in folder:\n",
    "        # check if current path is a file\n",
    "        if files != \"README.txt\":\n",
    "            filePath = os.path.join(path, files)\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                res.append(lines)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b16391c-fc5b-4834-a729-2035782bee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[t]\\n', 'Works[+3]##Works great, no odor, and uses regular bags.\\n', \"##Can't complain at all!\\n\", '[t]\\n', \"Diaper Champ[+2]##So far (3 weeks), we've had no problems with the Diaper Champ at all.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "res = read_data(path)\n",
    "doc = \"\"\n",
    "for a in res:\n",
    "    for b in a:\n",
    "        doc += b\n",
    "print(res[0][:5])\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac9c38aa-5974-4136-9dc0-0996730a0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def process_document(document: str) -> list:\n",
    "        \"\"\" pre-process a document and return a list of its terms: str->list\"\"\"\n",
    "        \n",
    "        # Remove number\n",
    "        text_nonum = re.sub(r'\\d+', ' ', document)\n",
    "        \n",
    "        pattern = r'''(?x)        # set flag to allow verbose regexps\n",
    "                    (?:[A-Z]\\.)+     #abbreviations\n",
    "                    |\\[\n",
    "                    |[^\\w\\s]\n",
    "                    |\\#\n",
    "                    |[-.(]+           #double hyphen, ellipsis, open parenthesis\n",
    "                    |\\S\\w*\n",
    "                    |\\$?\\d+(?:\\.\\d+)?%? #currency and percentages\n",
    "        '''\n",
    "        #Tokenization\n",
    "        tokenList = nltk.regexp_tokenize(text_nonum, pattern)\n",
    "        #To lower case\n",
    "        tokenList = [word.lower() for word in tokenList]\n",
    "        #Remove Punctuation\n",
    "        tokenList = list(filter(lambda word: punkt.PunktToken(word).is_non_punct,tokenList))\n",
    "        #Remove stopwords\n",
    "        stopW = stopwords.words(\"english\")\n",
    "        stopW.append(\"u\")\n",
    "        stopW.append(\"p\")\n",
    "        # stopW.append(\"mp\")\n",
    "        tokenList = list(filter(lambda word: word not in stopW,tokenList))\n",
    "        # Lemmatisation \n",
    "        lemma = WordNetLemmatizer()\n",
    "        tokenList = [lemma.lemmatize(word) for word in tokenList]\n",
    "\n",
    "        return tokenList\n",
    "\n",
    "def process_reviews_str(res: list) -> list:\n",
    "    # merge all reviews\n",
    "    doc = \"\"\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            doc += b\n",
    "    # Pre-process documents        \n",
    "    producedDoc = process_document(doc)\n",
    "    return producedDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeaebacc-04e8-4736-96ad-2b5a34c19981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top50(res: list) -> list:\n",
    "    doc = \"\"\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            doc += b\n",
    "    # Pre-process documents        \n",
    "    producedDoc = process_document(doc)\n",
    "    # Find the 50 most frequently occurred words\n",
    "    # Get the frequency of each word\n",
    "    word_frequencies = FreqDist(producedDoc)\n",
    "    # Sort the dictionary by frequency\n",
    "    sorted_frequencies = sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Select the top 50 words\n",
    "    target_words = [item[0] for item in sorted_frequencies[:50]]\n",
    "    \n",
    "    return target_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b02ead-22ba-486c-bae0-f2e48d1d4693",
   "metadata": {},
   "source": [
    "# Task 1 - Step 2 - Sample and Pseudowords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b14f5f2-4480-43e9-8241-22814cc70b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "# Reading documents\n",
    "res = read_data(path)\n",
    "processed_str = process_reviews_str(res)\n",
    "# print(processed_str[:10])\n",
    "target_words = get_top50(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ebc9d86-d798-47f8-b6de-f0cf440a288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pseudowords(target_words: list, processed_corpus: list) -> list:\n",
    "    # Construct a dic to store all the words' position\n",
    "    pos_dic = {}\n",
    "    for i, word in enumerate(processed_corpus):\n",
    "        pos_dic[i] = word\n",
    "    # print(\"\\n BEFORE\",pos_dic)\n",
    "    \n",
    "    # Sample half of the target words\n",
    "    sample_size = len(target_words) // 2\n",
    "    sample = random.sample(target_words, sample_size)\n",
    "    \n",
    "    # Create pseudowords for the sampled target words\n",
    "    madeups = [word[::-1] for word in sample]\n",
    "    \n",
    "    # Replace the sampled reversed words in the target words\n",
    "    pseudowords = target_words[:]\n",
    "    for i, word in enumerate(target_words):\n",
    "        if word in sample:\n",
    "            # Find the index of the word to be replace\n",
    "            replacement_index = sample.index(word)\n",
    "            # Replace the word with the corresponding word from the replacement list\n",
    "            pseudowords[i] = madeups[replacement_index]\n",
    "    \n",
    "    # replace half of its occurence in corpus with their reversed words\n",
    "    # Get the position of all the sample words\n",
    "    positions = []\n",
    "    for s in sample:\n",
    "        \n",
    "        pos_list = []\n",
    "        for index, word in pos_dic.items():\n",
    "            if s == word:\n",
    "                pos_list.append(index) \n",
    "        # print(\"\\n\",s, pos_list)\n",
    "        positions.append(pos_list)\n",
    "\n",
    "    for i, pos_list in enumerate(positions):\n",
    "        # Randomly select to be replaced words' index\n",
    "        samp_size = len(pos_list) // 2\n",
    "        sample_pos = random.sample(pos_list, samp_size)\n",
    "        # Replace all the random occurences to reversed words\n",
    "        for index in sample_pos:\n",
    "            pos_dic[index] = madeups[i]\n",
    "    \n",
    "    pseudo_corpus = []\n",
    "    for word in pos_dic.values():\n",
    "        pseudo_corpus.append(word)\n",
    "        \n",
    "    return pseudowords, pseudo_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25ede56-b821-4fac-90b3-21ff7aba49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_words, new_corpus = pseudowords(target_words, processed_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c73a0268-c8c4-41fa-a3c6-e4c298a25781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Words: \n",
      " ['use', 'phone', 'one', 'router', 'ipod', 'camera', 'player', 'get', 'battery', 'diaper', 'product', 'work', 'like', 'great', 'time', 'feature', 'problem', 'good', 'quality', 'zen', 'would', 'also', 'sound', 'computer', 'software', 'picture', 'well', 'really', 'micro', 'take', 'easy', 'thing', 'even', 'first', 'used', 'need', 'creative', 'bag', 'much', 'want', 'better', 'champ', 'mp', 'look', 'go', 'size', 'music', 'norton', 'little', 'price']\n",
      "Pseudo Words: \n",
      " ['esu', 'enohp', 'one', 'retuor', 'dopi', 'camera', 'reyalp', 'get', 'yrettab', 'diaper', 'tcudorp', 'work', 'ekil', 'taerg', 'time', 'feature', 'problem', 'good', 'ytilauq', 'zen', 'would', 'osla', 'dnuos', 'computer', 'erawtfos', 'picture', 'llew', 'yllaer', 'orcim', 'ekat', 'easy', 'gniht', 'even', 'first', 'desu', 'need', 'creative', 'gab', 'hcum', 'want', 'better', 'champ', 'pm', 'look', 'go', 'size', 'cisum', 'norton', 'elttil', 'ecirp']\n"
     ]
    }
   ],
   "source": [
    "print(\"Target Words: \\n\",target_words)\n",
    "print(\"Pseudo Words: \\n\",pseudo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e133a43c-3b0a-4cb0-9450-7fc9f91b2410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'work', 'great', 'odor', 'us', 'regular', 'bag', 'complain', 'diaper', 'champ', 'far', 'week', 'problem', 'diaper', 'champ', 'diaper', 'contains', 'smell', 'baby', 'diaper', 'use', 'kind', 'bag', 'inside', 'also', 'sprinkled', 'baking', 'soda', 'bottom', 'diaper', 'champ', 'help', 'absorb', 'odor', 'every', 'awhile', 'empty', 'old', 'baking', 'soda', 'replace', 'odor', 'refill', 'know', 'run', 'trouble', 'road', 'odor', 'far', 'complaint', 'happy', 'buy', 'refill', 'started', 'diaper', 'genie', 'new', 'parent', 'hated', 'thing', 'new', 'design', 'still', 'need', 'lot', 'improvement', 'bag', 'never', 'fit', 'right', 'never', 'spun', 'right', 'two', 'month', 'sat', 'corner', 'room', 'never', 'used', 'entered', 'toddlerhood', 'started', 'bad', 'diaper', 'becasue', 'cutting', 'teeth', 'diet', 'change', 'decided', 'something', 'started', 'researching', 'diaper', 'champ', 'diaper', 'champ', 'best', 'found']\n"
     ]
    }
   ],
   "source": [
    "print(processed_str[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b2ab13d-bbae-474c-ad58-f42df6f6e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'work', 'great', 'odor', 'us', 'regular', 'bag', 'complain', 'diaper', 'champ', 'far', 'week', 'problem', 'diaper', 'champ', 'diaper', 'contains', 'smell', 'baby', 'diaper', 'use', 'kind', 'bag', 'inside', 'also', 'sprinkled', 'baking', 'soda', 'bottom', 'diaper', 'champ', 'help', 'absorb', 'odor', 'every', 'awhile', 'empty', 'old', 'baking', 'soda', 'replace', 'odor', 'refill', 'know', 'run', 'trouble', 'road', 'odor', 'far', 'complaint', 'happy', 'buy', 'refill', 'started', 'diaper', 'genie', 'new', 'parent', 'hated', 'thing', 'new', 'design', 'still', 'need', 'lot', 'improvement', 'gab', 'never', 'fit', 'right', 'never', 'spun', 'right', 'two', 'month', 'sat', 'corner', 'room', 'never', 'desu', 'entered', 'toddlerhood', 'started', 'bad', 'diaper', 'becasue', 'cutting', 'teeth', 'diet', 'change', 'decided', 'something', 'started', 'researching', 'diaper', 'champ', 'diaper', 'champ', 'best', 'found']\n"
     ]
    }
   ],
   "source": [
    "print(new_corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14119467-822c-4ef7-ba01-fd2fb28599b8",
   "metadata": {},
   "source": [
    "# Task 1 - Step 3 - Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f827dd0-89f9-4d3a-8ab8-f84d70b64731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVector(processed_list: list, sample: str) -> np.array:\n",
    "    \n",
    "    sg_model = Word2Vec(processed_list, min_count = 1, vector_size = 100, window = 5, sg = 1) \n",
    "    # cbow_model = Word2Vec(processed_list, min_count = 1, vector_size = 100, window = 5)\n",
    "    \n",
    "    M = []\n",
    "    for s in sample:\n",
    "        vec = sg_model.wv[s]\n",
    "        M.append(vec)\n",
    "    \n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95e30091-3dd8-4d55-bd7a-42b668549f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'use' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_24064/3447962254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpseudo_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_24064/1265318265.py\u001b[0m in \u001b[0;36mfeatureVector\u001b[0;34m(processed_list, sample)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'use' not present\""
     ]
    }
   ],
   "source": [
    "sample = target_words + pseudo_words\n",
    "print(new_corpus.index(\"use\"))\n",
    "M = featureVector(new_corpus, sample)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f46b9-8301-4bd9-b036-98a5d52c2c29",
   "metadata": {},
   "source": [
    "# Task 1 - Step 4 - input matrix X put them into 50 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89e1dc-c92a-4f65-8f37-5cfb6398fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.cluster import KMeansClusterer\n",
    "\n",
    "# X = M\n",
    "# # Set the number of clusters to 50\n",
    "# num_clusters = 50\n",
    "\n",
    "# # Create a KMeansClusterer instance with the specified number of clusters\n",
    "# clusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, avoid_empty_clusters=True)\n",
    "\n",
    "# # Cluster the words using the feature matrix\n",
    "# clusters = clusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "# # Print the cluster assignments for each word\n",
    "# for i, cluster in enumerate(clusters):\n",
    "#     print(f\"Word {i+1} is in cluster {cluster}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09779a14-e6aa-4fc6-ac4a-095a01cb3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# X = M\n",
    "\n",
    "# # Set the number of clusters to 50\n",
    "# num_clusters = 50\n",
    "\n",
    "# # Create a KMeans instance with the specified number of clusters\n",
    "# kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "\n",
    "# # Cluster the words using the feature matrix\n",
    "# clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# visualClusters = {}\n",
    "# # Print the cluster assignments for each word\n",
    "# for i, cluster in enumerate(clusters):\n",
    "# #    print(f\"Word {i+1} is in cluster {cluster}.\")\n",
    "#     visualClusters[corpus[i]] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93060f-3ee7-4d03-8c28-8ef051ed5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getCluster(X: np.array) -> dict:\n",
    "#     # Set the number of clusters to 50\n",
    "#     num_clusters = 50\n",
    "\n",
    "#     # Create a KMeans instance with the specified number of clusters\n",
    "#     kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "\n",
    "#     # Cluster the words using the feature matrix\n",
    "#     clusters = kmeans.fit_predict(X)\n",
    "\n",
    "#     visualClusters = {}\n",
    "#     # Print the cluster assignments for each word\n",
    "#     for i, cluster in enumerate(clusters):\n",
    "#     #    print(f\"Word {i+1} is in cluster {cluster}.\")\n",
    "#         visualClusters[corpus[i]] = cluster\n",
    "#     return visualClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09403be-61c9-45f9-a0a2-c2fffe06fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCluster(X: np.array) -> np.array:\n",
    "    # Set the number of clusters to 50\n",
    "    num_clusters = 50\n",
    "    \n",
    "    # Create a KMeans instance with the specified number of clusters\n",
    "    # kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "    km = KMeans(n_clusters=50).fit(X)\n",
    "    \n",
    "    # Cluster the words using the feature matrix\n",
    "    # clusters = km.fit_predict(X)\n",
    "    performance = []\n",
    "    \n",
    "    labels = km.labels_\n",
    "    labels.tolist()\n",
    "    performance.append((np.sum(labels[0:50] == labels[50:100])) / len(labels[0:50]))\n",
    "    print(\"target words: \" + str(labels[50:100]))\n",
    "    print(\"pesudo words: \" + str(labels[0:50]))\n",
    "    print(\"Performance: \" + str(performance[0]))\n",
    "        \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c545fe-db42-45dc-aa54-4ab77295f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "getCluster(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4434a48-8569-40b6-93bf-0cc88004d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c24c59-9e22-4915-8797-1192a1240879",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = getCluster(M)\n",
    "print(clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2531614-b026-46e5-b3eb-9d1392bd0c7b",
   "metadata": {},
   "source": [
    "# Task 1 - Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8e302-2700-4375-9dd2-410bae6d0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_cluster = clust[:50]\n",
    "# pseudo_cluster = clust[50:]\n",
    "# print(target_cluster,len(target_cluster),\"\\n\",pseudo_cluster, len(pseudo_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab61d18-3e35-4c02-ba8d-c38ddfad6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pseudowords, len(pseudowords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ceb0c-ca3f-4726-bc53-aac03726da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_words, len(target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf29146-1dbd-4ef1-90b2-0f373c37cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_size = 50\n",
    "# temp_count = 0\n",
    "# for i in range(cluster_size):\n",
    "#     if(target_cluster[i] == pseudo_cluster[i]):\n",
    "#         temp_count += 1\n",
    "# p = temp_count/cluster_size\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75191e-fe7b-4209-9990-754b10976cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(clust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab31dc8-ec8f-4d60-84b4-89f7af9c7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getProbability(cluster: np.array) -> int:\n",
    "#     cluster_size = len(cluster) // 2 \n",
    "#     temp_count = 0\n",
    "#     for i in range(cluster_size):\n",
    "#         if(target_cluster[i] == pseudo_cluster[i]):\n",
    "#             temp_count += 1\n",
    "#     p = temp_count/cluster_size\n",
    "#     return p\n",
    "# getProbability(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d48a7c-292b-403f-8fb2-243e7275522f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
