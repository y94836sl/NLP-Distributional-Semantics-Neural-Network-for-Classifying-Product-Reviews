{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rubyli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/rubyli/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# For file reading\n",
    "import os\n",
    "from os import listdir\n",
    "# For pre-processing \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "# For skip-gram model\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# For Cluster\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90bad112-f093-4a91-9de6-e67a56cdc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> list:\n",
    "    \"\"\" Read files from a directory and then append the data of each file into a list. \"\"\"\n",
    "    folder = listdir(path)\n",
    "    res = []\n",
    "    for files in folder:\n",
    "        # check if current path is a file\n",
    "        if files != \"README.txt\":\n",
    "            filePath = os.path.join(path, files)\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                res.append(lines)\n",
    "    return res\n",
    "\n",
    "def process_document(document: str) -> list:\n",
    "        \"\"\" pre-process a document and return a list of its terms: str->list\"\"\"\n",
    "        \n",
    "        # Remove number\n",
    "        text_nonum = re.sub(r'\\d+', ' ', document)\n",
    "        \n",
    "        pattern = r'''(?x)        # set flag to allow verbose regexps\n",
    "                    (?:[A-Z]\\.)+     #abbreviations\n",
    "                    |\\[\n",
    "                    |[^\\w\\s]\n",
    "                    |\\#\n",
    "                    |[-.(]+           #double hyphen, ellipsis, open parenthesis\n",
    "                    |\\S\\w*\n",
    "                    |\\$?\\d+(?:\\.\\d+)?%? #currency and percentages\n",
    "        '''\n",
    "        #Tokenization\n",
    "        tokenList = nltk.regexp_tokenize(text_nonum, pattern)\n",
    "        #To lower case\n",
    "        tokenList = [word.lower() for word in tokenList]\n",
    "        #Remove Punctuation\n",
    "        tokenList = list(filter(lambda word: punkt.PunktToken(word).is_non_punct,tokenList))\n",
    "        #Remove stopwords\n",
    "        stopW = stopwords.words(\"english\")\n",
    "        stopW.append(\"u\")\n",
    "        stopW.append(\"p\")\n",
    "        # stopW.append(\"mp\")\n",
    "        tokenList = list(filter(lambda word: word not in stopW,tokenList))\n",
    "        # Lemmatisation \n",
    "        lemma = WordNetLemmatizer()\n",
    "        tokenList = [lemma.lemmatize(word) for word in tokenList]\n",
    "\n",
    "        return tokenList \n",
    "\n",
    "def process_reviews_str(res: list) -> list:\n",
    "    # merge all reviews\n",
    "    doc = \"\"\n",
    "    for a in res:\n",
    "        for b in a:\n",
    "            doc += b\n",
    "    # Pre-process documents        \n",
    "    producedDoc = process_document(doc)\n",
    "    return producedDoc\n",
    "\n",
    "\n",
    "def get_top50(producedDoc: str) -> list:\n",
    "    # Find the 50 most frequently occurred words\n",
    "    # Get the frequency of each word\n",
    "    word_frequencies = FreqDist(producedDoc)\n",
    "    # Sort the dictionary by frequency\n",
    "    sorted_frequencies = sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Select the top 50 words\n",
    "    target_words = [item[0] for item in sorted_frequencies[:50]]\n",
    "    \n",
    "    return target_words\n",
    "\n",
    "def pseudowords(target_words: list, processed_corpus: list) -> list:\n",
    "    # Sample half of the target words\n",
    "    sample_size = len(target_words) // 2\n",
    "    sample = random.sample(target_words, sample_size)\n",
    "    \n",
    "    \n",
    "    # Create pseudowords for the sampled target words\n",
    "    madeups = [word[::-1] for word in sample]\n",
    "    # Replace the sampled occurrences of the target words with their pseudowords\n",
    "    pseudowords = target_words[:]\n",
    "    pseudo_corpus = processed_corpus[:]\n",
    "    for i, word in enumerate(target_words):\n",
    "        if word in sample:\n",
    "            # Find the index of the word to be replace\n",
    "            replacement_index = sample.index(word)\n",
    "            # Replace the word with the corresponding word from the replacement list\n",
    "            pseudowords[i] = madeups[replacement_index]\n",
    "    return pseudowords, pseudo_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d43998b6-44f6-4606-b719-c72f08e2caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVector(processed_list: list, sample: str) -> np.array:\n",
    "    \n",
    "    cbow_model = Word2Vec(processed_list, min_count = 1, vector_size = 100, window = 5)\n",
    "    word_vectors = cbow_model.wv.vectors\n",
    "    # print(list(cbow_model.wv.key_to_index))\n",
    "    \n",
    "    M = []\n",
    "    for s in sample:\n",
    "        vec = cbow_model.wv[s]\n",
    "        M.append(vec)\n",
    "    \n",
    "    # M = np.array(word_vectors)\n",
    "    return M\n",
    "#     M = []\n",
    "#     for s in sample:\n",
    "#         word_vectors = cbow_model.wv[s]\n",
    "#         M.append(word_vectors)\n",
    "        \n",
    "#     print(M)\n",
    "\n",
    "\n",
    "#     # svd\n",
    "#     # u, s, v = np.linalg.svd(positive_skip_grams)\n",
    "#     u, s, v = np.linalg.svd(word_vectors)\n",
    "#     # u, s, v = np.linalg.svd(M)\n",
    "#     M = np.dot(u, np.diag(s))\n",
    "    \n",
    "#     # norm\n",
    "#     M = M / np.linalg.norm(M, axis=1)[:, None]\n",
    "#     return M\n",
    "\n",
    "def getCluster(X: np.array) -> np.array:\n",
    "    # Set the number of clusters to 50\n",
    "    num_clusters = 50\n",
    "    \n",
    "    # Create a KMeans instance with the specified number of clusters\n",
    "    km = KMeans(n_clusters=50).fit(X)\n",
    "    \n",
    "    performance = []\n",
    "    \n",
    "    labels = km.labels_\n",
    "    labels.tolist()\n",
    "    performance.append((np.sum(labels[0:50] == labels[50:100])) / len(labels[0:50]))\n",
    "    print(\"target words: \" + str(labels[50:100]))\n",
    "    print(\"pesudo words: \" + str(labels[0:50]))\n",
    "    print(\"Performance: \" + str(performance[0]))\n",
    "        \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ebc9d86-d798-47f8-b6de-f0cf440a288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb6a3df5-3bc8-4faf-ac52-c33dd96aeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "# Reading documents\n",
    "res = read_data(path)\n",
    "processed_str = process_reviews_str(res)\n",
    "processed_list = process_reviews_list(res)\n",
    "target_words = get_top50(processed_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4bae81d-dbde-410f-82d9-d7e4d53e9657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['krow', 'work', 'taerg', 'odor', 'us', 'regular', 'gab', 'complain', 'diaper', 'pmahc']\n"
     ]
    }
   ],
   "source": [
    "# pseudo_words = pseudowords(target_words)\n",
    "pseudo_words, new_corpus = pseudowords(target_words, processed_str)\n",
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59e28b58-026b-4bf1-acfb-9a109ea5f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3bd72d5f-2cd6-41aa-baf2-19df30f06e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pseudo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1331a6d-0018-4be6-878b-cdbc588d3535",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'use' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_9962/670370862.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# clust = getCluster(M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpseudo_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_9962/3622958361.py\u001b[0m in \u001b[0;36mfeatureVector\u001b[0;34m(processed_list, sample)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'use' not present\""
     ]
    }
   ],
   "source": [
    "# M = featureVector(target_words, pseudo_words, res)\n",
    "# clust = getCluster(M)\n",
    "sample = target_words + pseudo_words\n",
    "M = featureVector(new_corpus, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57c5d27c-e8ea-493a-a1b4-1a192c2d7b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5186, 100)\n"
     ]
    }
   ],
   "source": [
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1af4a5-9ff3-4d1f-8ab8-09c011f99ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ddcf02-8c0e-42da-ab7d-6c81b78ddb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = getProbability(clust)\n",
    "# print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
