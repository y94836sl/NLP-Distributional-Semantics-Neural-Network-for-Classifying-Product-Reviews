{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b18efd-fc1f-457f-ae73-34397493211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "import nltk\n",
    "\n",
    "# For file reading\n",
    "import os\n",
    "from os import listdir\n",
    "# For pre-processing \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7e5886-c4ee-44de-89e2-09f3baef009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> list:\n",
    "    \"\"\" Read files from a directory and then append the data of each file into a list. \"\"\"\n",
    "    folder = listdir(path)\n",
    "    res = []\n",
    "    for files in folder:\n",
    "        # check if current path is a file\n",
    "        if files != \"README.txt\":\n",
    "            filePath = os.path.join(path, files)\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                res.append(lines)\n",
    "    return res\n",
    "\n",
    "def get_raw_data(res: list) -> list:\n",
    "    '''Get avaliable data for training and testing data'''\n",
    "    # Getting avaliable data for LSTM\n",
    "    avaliable_data = []\n",
    "    for document in res:\n",
    "        # Remove \\n from the string list\n",
    "        processedList = [s.strip() for s in document]\n",
    "        # Remove [t] tag\n",
    "        processedList = [re.sub('\\[t\\]', '', word) for word in processedList]\n",
    "        # Store values in list\n",
    "        for sentence in processedList:\n",
    "            # Remove non-tagged data and empty data\n",
    "            if (not sentence.startswith(\"##\")) and (sentence != \"\"):\n",
    "                avaliable_data.append(sentence)\n",
    "                \n",
    "    # Store raw review and raw sentiment seperatly in 2 lists\n",
    "    raw_tag = []\n",
    "    raw_text = []\n",
    "    \n",
    "    for data in avaliable_data:\n",
    "        # Split data into tag and text\n",
    "        split_data = data.split(\"##\")\n",
    "\n",
    "        left_tag = split_data[0]\n",
    "        right_text = split_data[-1]\n",
    "\n",
    "        # print(\"Before:   \",left_tag)\n",
    "\n",
    "        # Get pre-defined sentiment tags, with or without +/-\n",
    "        num_list = re.findall(r'[+|-]+\\d+', left_tag)\n",
    "        num_only = re.findall(r'\\[\\d+\\]', left_tag)\n",
    "\n",
    "        new_num_only = [num.replace('[', '') for num in num_only]\n",
    "        new_num_only = [num.replace(']', '') for num in new_num_only]\n",
    "        num_list = num_list +  new_num_only\n",
    "        \n",
    "        raw_tag.append(num_list)\n",
    "        raw_text.append(right_text)\n",
    "    \n",
    "    return raw_tag, raw_text\n",
    "\n",
    "def preprocess_text(s: str) -> list:\n",
    "    '''Text cleaning and pre-processing'''\n",
    "    # Remove number\n",
    "    s = re.sub(r'\\d+', ' ', s)\n",
    "    pattern = r'''(?x)        # set flag to allow verbose regexps\n",
    "                    (?:[A-Z]\\.)+      #abbreviations, e.g. \n",
    "                    |\\[\n",
    "                    |\\w+\\'+\\w+        #keep contractions, e.g. doesn't \n",
    "                    |[^\\w\\s]\n",
    "                    |\\#\n",
    "                    |[-.(]+           #double hyphen, ellipsis, open parenthesis\n",
    "                    |\\S\\w*\n",
    "                    |\\$?\\d+(?:\\.\\d+)?%? #currency and percentages\n",
    "        '''\n",
    "    # Tokenization\n",
    "    token_list = nltk.regexp_tokenize(s, pattern)\n",
    "    #To lower case\n",
    "    token_list = [word.lower() for word in token_list]\n",
    "    #Remove stopwords\n",
    "    stopW = stopwords.words(\"english\")\n",
    "    stopW.append(\"mp\")\n",
    "    stopW.append(\"i'm\")\n",
    "    stopW.append(\"i've\")\n",
    "    token_list = list(filter(lambda word: word not in stopW,token_list))\n",
    "    #Remove Punctuation\n",
    "    token_list = list(filter(lambda word: punkt.PunktToken(word).is_non_punct,token_list))\n",
    "    # Lemmatization \n",
    "    lemma = WordNetLemmatizer()\n",
    "    token_list = [lemma.lemmatize(word) for word in token_list]\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "def get_review_label(raw_tag: list, raw_text: list):\n",
    "    '''Pre-process the reviews and encoding the label'''\n",
    "    reviews = []\n",
    "    for text in raw_text:\n",
    "        review = preprocess_text(text)\n",
    "        reviews.append(review)\n",
    "    sentiments = []\n",
    "    for review in raw_tag:\n",
    "        total = 0\n",
    "        tag_size = len(review)\n",
    "        # Add all the positive and negative tag together to check if the whole review is positive or negative\n",
    "        if tag_size >= 1:\n",
    "            for num in review:\n",
    "                total += int(num)\n",
    "        \n",
    "        # Encoded the labels\n",
    "        if total > 0:\n",
    "            sentiments.append(1)\n",
    "        else:\n",
    "            sentiments.append(0)\n",
    "    return sentiments, reviews\n",
    "\n",
    "\n",
    "def get_encoded_reviews(reviews: list):\n",
    "    '''Encoding all the words with their frquency'''\n",
    "    # Get word frequency in all reviews\n",
    "    all_word = []\n",
    "    for sen in reviews:\n",
    "        all_word+=sen\n",
    "\n",
    "    word_freq = nltk.FreqDist(all_word)\n",
    "\n",
    "    # Encode the reviews\n",
    "    encoded_review = []\n",
    "    for sen in reviews:\n",
    "        encode_list = list(word_freq[word] for word in sen)\n",
    "        encoded_review.append(encode_list)\n",
    "    \n",
    "    return encoded_review\n",
    "\n",
    "\n",
    "def pad_features(reviews_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8647ef7d-7cf9-42dd-9a95-34152c93839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rubyli/Desktop/GitHubRepos/UoM/NLP-Distributional-Semantics-Neural-Network-for-Classifying-Product-Reviews/product_reviews\"\n",
    "\n",
    "# Reading documents\n",
    "res = read_data(path)\n",
    "raw_tag, raw_text = get_raw_data(res)\n",
    "labels, texts = get_review_label(raw_tag, raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee895fe-54d5-4365-84ab-50b733a876e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_12553/4028242596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create the dataframe for training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiments' is not defined"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(sentiments)):\n",
    "    data.append([reviews[i], sentiments[i]])\n",
    "# print(data)\n",
    "# Create the dataframe for training and testing data\n",
    "df = pd.DataFrame(data, columns=[\"Reviews\", \"Sentiments\"])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab618962-2bb1-4378-84d1-6805ff3478b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2138\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a815a4-5da8-42aa-8e6a-d877a7078a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115, 154, 28, 11, 16, 62], [42, 17, 5, 128, 108, 51], [1, 22, 2, 108, 190, 13, 62, 14], [25, 2, 19, 7, 1, 28, 42, 12, 24, 43, 19], [108, 51, 53, 23], [1, 62, 14, 43], [190, 15, 1, 2, 2, 62, 2, 18, 190, 62, 1, 24], [34, 1, 5, 108, 3, 5], [5, 28, 35, 30, 28], [42, 77, 15, 108, 17, 5, 1, 13, 93, 42]]\n"
     ]
    }
   ],
   "source": [
    "encoded_reviews = get_encoded_reviews(texts)\n",
    "print(encoded_reviews[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "810b3857-75c9-4848-9036-12577fee0489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    2138.000000\n",
       "mean        8.902713\n",
       "std         5.399257\n",
       "min         0.000000\n",
       "25%         5.000000\n",
       "50%         8.000000\n",
       "75%        11.000000\n",
       "max        48.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_len = [len(x) for x in encoded_reviews]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(reviews_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca4633b-ac83-4209-890e-a3153b2a39c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  11  16  62]\n",
      " [  0   0   0 ... 128 108  51]\n",
      " [  0   0   0 ...  13  62  14]\n",
      " ...\n",
      " [  0   0   0 ...  24   2  17]\n",
      " [  0   0   0 ...   4  40   5]\n",
      " [  0   0   0 ...   2  46  35]]\n"
     ]
    }
   ],
   "source": [
    "squence_len = 30\n",
    "features = pad_features(encoded_reviews, squence_len)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e711f3e2-7d75-4610-98c5-109102745beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2138\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(features))\n",
    "print(len(features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93543a27-f423-4668-b8ee-d62ab08d88ef",
   "metadata": {},
   "source": [
    "# Split data into training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c009fd-ece8-4af3-95a7-32ec7e7d0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training data and testing data\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(features, labels, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0e997f-bbbc-441d-9d85-933d62d7c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603 535 1603 535\n"
     ]
    }
   ],
   "source": [
    "print(len(texts_train), len(texts_test), len(labels_train), len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477a6f9-e2a5-49b6-be8c-988301278e9c",
   "metadata": {},
   "source": [
    "# Batching and loading as tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b88394-a792-411b-aea3-aff8fa41118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 30])\n",
      "Sample input: \n",
      " tensor([[  0,   0,   0,  ...,  27,   7,  48],\n",
      "        [  0,   0,   0,  ...,   1, 102,  38],\n",
      "        [  0,   0,   0,  ...,   0,  42,  27],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ...,  17,  33,   1],\n",
      "        [  0,   0,   0,  ..., 115,   3, 115],\n",
      "        [  0,   0,   0,  ..., 118,  87,  71]])\n",
      "Sample input: \n",
      " tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1])\n"
     ]
    }
   ],
   "source": [
    "texts_train = np.array(texts_train)\n",
    "labels_train = np.array(labels_train)\n",
    "texts_test = np.array(texts_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(texts_train), torch.from_numpy(labels_train))\n",
    "test_data = TensorDataset(torch.from_numpy(texts_test), torch.from_numpy(labels_test))\n",
    "  \n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# SHUFFLE the data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe57db9-5f1c-4375-966a-16ef0a3d4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_x size:    50\n",
      "sample_y size:    50\n"
     ]
    }
   ],
   "source": [
    "print(\"sample_x size:   \", len(sample_x))\n",
    "print(\"sample_y size:   \", len(sample_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73625fc0-b4bb-4d39-9577-2ad9c1c271e6",
   "metadata": {},
   "source": [
    "# Define the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91802ca2-73d7-4ab6-8a15-5258ca3e44f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c7/ncftj36n1_7g9ljr1hpqgvj00000gn/T/ipykernel_12553/2735165493.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_dim = 50\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
    "lstm_out = 32\n",
    "model.add(Bidirectional(LSTM(lstm_out)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50e7ca-b058-4993-8ba1-8a16a68b66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 1234\n",
    "\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# class SentimentLSTM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     The RNN model that will be used to perform Sentiment analysis.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "#         \"\"\"\n",
    "#         Initialize the model by setting up the layers.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.output_size = output_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.hidden_dim = hidden_dim\n",
    "        \n",
    "#         # embedding and LSTM layers\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "#                             dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "#         # dropout layer\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "#         # linear and sigmoid layers\n",
    "#         self.fc = nn.Linear(hidden_dim, output_size)\n",
    "#         self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "#     def forward(self, x, hidden):\n",
    "#         \"\"\"\n",
    "#         Perform a forward pass of our model on some input and hidden state.\n",
    "#         \"\"\"\n",
    "#         batch_size = x.size(0)\n",
    "\n",
    "#         # embeddings and lstm_out\n",
    "#         embeds = self.embedding(x)\n",
    "#         lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "#         # stack up lstm outputs\n",
    "#         lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "#         # dropout and fully-connected layer\n",
    "#         out = self.dropout(lstm_out)\n",
    "#         out = self.fc(out)\n",
    "#         # sigmoid function\n",
    "#         sig_out = self.sig(out)\n",
    "        \n",
    "#         # reshape to be batch_size first\n",
    "#         sig_out = sig_out.view(batch_size, -1)\n",
    "#         sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "#         # return last sigmoid output and hidden state\n",
    "#         return sig_out, hidden\n",
    "    \n",
    "    \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         ''' Initializes hidden state '''\n",
    "#         # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "#         # initialized to zero, for hidden state and cell state of LSTM\n",
    "#         weight = next(self.parameters()).data\n",
    "        \n",
    "#         # if (train_on_gpu):\n",
    "#         #     hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "#         #           weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "#         # else:\n",
    "#         hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "#                       weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "#         return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de71f9-d94c-4706-b228-5b6f1dda4a4e",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8645d23-f050-45b9-bfaa-9302169636c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Instantiate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0e529-f5bc-4110-9578-7b396c7516a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with hyperparams\n",
    "# vocab_size = len(encoded_reviews)+1 # +1 for the 0 padding\n",
    "# output_size = 1\n",
    "# embedding_dim = 400\n",
    "# hidden_dim = 256\n",
    "# n_layers = 2\n",
    "# net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3b7f6-b041-4f01-bc4e-33760a4ab264",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c2d16-748a-4226-bbb5-95a315273831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss and optimization functions\n",
    "# lr=0.001\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# # training params\n",
    "\n",
    "# epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "# counter = 0\n",
    "# print_every = 100\n",
    "# clip=5 # gradient clipping\n",
    "\n",
    "# # # move model to GPU, if available\n",
    "# # if(train_on_gpu):\n",
    "# #     net.cuda()\n",
    "\n",
    "# net.train()\n",
    "# # train for some number of epochs\n",
    "# for e in range(epochs):\n",
    "#     # initialize hidden state\n",
    "#     h = net.init_hidden(batch_size)\n",
    "\n",
    "#     # batch loop\n",
    "#     for inputs, labels in train_loader:\n",
    "#         counter += 1\n",
    "\n",
    "#         # if(train_on_gpu):\n",
    "#         #     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "#         # Creating new variables for the hidden state, otherwise\n",
    "#         # we'd backprop through the entire training history\n",
    "#         h = tuple([each.data for each in h])\n",
    "\n",
    "#         # zero accumulated gradients\n",
    "#         net.zero_grad()\n",
    "\n",
    "#         # get the output from the model\n",
    "#         inputs = inputs.type(torch.LongTensor)\n",
    "#         output, h = net(inputs, h)\n",
    "\n",
    "#         # calculate the loss and perform backprop\n",
    "#         loss = criterion(output.squeeze(), labels.float())\n",
    "#         loss.backward()\n",
    "#         # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "#         nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # loss stats\n",
    "#         if counter % print_every == 0:\n",
    "#             # Get validation loss\n",
    "#             val_h = net.init_hidden(batch_size)\n",
    "#             val_losses = []\n",
    "#             net.eval()\n",
    "#             for inputs, labels in valid_loader:\n",
    "\n",
    "#                 # Creating new variables for the hidden state, otherwise\n",
    "#                 # we'd backprop through the entire training history\n",
    "#                 val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "#                 # if(train_on_gpu):\n",
    "#                 #     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "#                 inputs = inputs.type(torch.LongTensor)\n",
    "#                 output, val_h = net(inputs, val_h)\n",
    "#                 val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "#                 val_losses.append(val_loss.item())\n",
    "\n",
    "#             net.train()\n",
    "#             print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "#                   \"Step: {}...\".format(counter),\n",
    "#                   \"Loss: {:.6f}...\".format(loss.item()),\n",
    "#                   \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b0527-82b7-43a1-905a-4c2ea5c26428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed597a25-3583-495c-b65d-08f3a6dfb2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
